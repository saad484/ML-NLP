{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP576oFeXoSjDOHsXGrOvrJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saad484/ML_NLP/blob/main/Analyse_des_sentiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EeHZP014fmk",
        "outputId": "6cf05961-0925-4001-d0de-007dd5b2b1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "x_train = pd.read_csv(\"/content/drive/MyDrive/Train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def supprimer_crochets(text):\n",
        " return re.sub('\\[[^]]*\\]', '', text)\n",
        "x_train['text']=x_train['text'].apply(supprimer_crochets)\n",
        "def supprimer_special(text, remove_digits=True):\n",
        " pattern=r'[^a-zA-z0-9\\s]'\n",
        " text=re.sub(pattern,'',text)\n",
        " return text\n",
        "#Apply function on review column\n",
        "x_train['text']=x_train['text'].apply(supprimer_special)\n",
        "import nltk\n",
        "nltk.download([\"names\",\"stopwords\",\"punkt\"])\n",
        "#Stemming the text\n",
        "def simple_stemmer(text):\n",
        " ps=nltk.porter.PorterStemmer()\n",
        " text= ' '.join([ps.stem(word) for word in text.split()])\n",
        " return text\n",
        "#Apply function on review column\n",
        "x_train['text']=x_train['text'].apply(simple_stemmer)\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer=ToktokTokenizer()\n",
        "stopword_list=nltk.corpus.stopwords.words('english')\n",
        "from nltk.corpus import stopwords\n",
        "stop=set(stopwords.words('english'))\n",
        "print(stop)\n",
        "def remove_stopwords(text):\n",
        " tokens = tokenizer.tokenize(text)\n",
        " tokens = [token.strip() for token in tokens]\n",
        " filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        " filtered_text = ' '.join(filtered_tokens)\n",
        " return filtered_text\n",
        "x_train['text']=x_train['text'].apply(remove_stopwords)\n",
        "#30000 commentaires pour l'entrainement\n",
        "norm_train_text=x_train.text[:30000]\n",
        "norm_train_text[0]\n",
        "#10000 commentaires pour le test\n",
        "norm_test_text=x_train.text[30000:]\n",
        "norm_test_text[35005]\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#Tfidf vectorizer\n",
        "tv=TfidfVectorizer()\n",
        "#transformed train reviews\n",
        "tv_train_text=tv.fit_transform(norm_train_text)\n",
        "#transformed test reviews\n",
        "tv_test_text=tv.transform(norm_test_text)\n",
        "print('Tfidf_train:',tv_train_text.shape)\n",
        "print('Tfidf_test:',tv_test_text.shape)\n",
        "sentiment_data = x_train['label']\n",
        "#Spliting the sentiment data (labels)\n",
        "train_sentiments=sentiment_data[:30000]\n",
        "test_sentiments=sentiment_data[30000:]\n",
        "print(train_sentiments)\n",
        "print(test_sentiments)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#training the model\n",
        "lr=LogisticRegression(penalty='l2',max_iter=500,C=1)\n",
        "#Fitting the model for tfidf features\n",
        "lr_tfidf=lr.fit(tv_train_text,train_sentiments)\n",
        "print(lr_tfidf)\n",
        "#Predicting the model for tfidf features\n",
        "lr_tfidf_predict=lr.predict(tv_test_text)\n",
        "print(lr_tfidf_predict)\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
        "print(\"lr_tfidf_score :\",lr_tfidf_score)\n",
        "confusion_matrix(test_sentiments,lr_tfidf_predict)\n",
        "#one comment prediction\n",
        "liste = []\n",
        "commentaire = \"hello it is a bad movie\"\n",
        "commentaire = supprimer_crochets(commentaire)\n",
        "commentaire = supprimer_special(commentaire)\n",
        "commentaire = simple_stemmer(commentaire)\n",
        "tokens = remove_stopwords(commentaire)\n",
        "liste.append(tokens)\n",
        "x = tv.transform(liste)\n",
        "res = lr.predict(x)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29gqAZpUFT05",
        "outputId": "5c8e829e-2b9e-4c83-c94c-ad393ec74990"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'haven', 'ours', 'about', 'a', 'how', 'should', 'o', 'if', 'same', 'so', \"shouldn't\", 'my', 'other', 'further', 'were', \"haven't\", \"you'd\", \"couldn't\", 'them', 'just', 'an', 'aren', 'between', 'i', 'each', 'when', 'own', 'she', \"aren't\", 'then', 'where', 'with', 'm', 'yourself', 'than', 't', 'wouldn', 'had', 'now', 'because', 'didn', 'above', 'isn', 'to', \"doesn't\", 'only', 'again', 've', 'ourselves', 'yourselves', \"needn't\", \"should've\", 'but', 'most', 'some', 'such', 'its', \"wouldn't\", 'their', 'mightn', 'any', 'here', 'out', \"you've\", 'ain', 'won', 'was', \"she's\", 'doesn', 'you', 'while', 'having', 'her', \"hasn't\", 'from', 'nor', 'which', 'once', 'y', 'has', 'under', 'shan', 'does', 'after', 'wasn', \"that'll\", 'don', 'shouldn', 'those', 'the', 'that', 'over', 'd', 'weren', 'off', 'too', 'themselves', \"weren't\", \"don't\", 'is', 'who', 'couldn', 'hadn', \"won't\", 'at', 'not', \"wasn't\", 'all', 'by', 'needn', 'he', \"mustn't\", 'it', \"it's\", 'they', 'for', 'your', 're', 'his', 'hasn', 'few', 'our', 'itself', 'did', 'whom', 'herself', 'until', \"mightn't\", 'more', 'we', 'through', 'be', 'against', 'himself', 's', 'll', 'during', 'been', \"you'll\", 'very', \"you're\", 'or', 'can', 'being', 'me', 'of', 'why', 'both', 'will', 'ma', 'into', 'before', 'are', 'down', 'what', 'mustn', 'no', 'am', 'these', 'as', 'doing', \"isn't\", 'theirs', 'have', 'do', 'in', \"shan't\", 'yours', 'myself', 'on', 'and', 'this', \"hadn't\", 'there', 'hers', 'him', \"didn't\", 'up', 'below'}\n",
            "Tfidf_train: (30000, 104419)\n",
            "Tfidf_test: (10000, 104419)\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        1\n",
            "        ..\n",
            "29995    1\n",
            "29996    1\n",
            "29997    1\n",
            "29998    0\n",
            "29999    1\n",
            "Name: label, Length: 30000, dtype: int64\n",
            "30000    0\n",
            "30001    1\n",
            "30002    0\n",
            "30003    0\n",
            "30004    0\n",
            "        ..\n",
            "39995    1\n",
            "39996    1\n",
            "39997    0\n",
            "39998    1\n",
            "39999    1\n",
            "Name: label, Length: 10000, dtype: int64\n",
            "LogisticRegression(C=1, max_iter=500)\n",
            "[0 1 0 ... 0 1 1]\n",
            "lr_tfidf_score : 0.8926\n",
            "[0]\n"
          ]
        }
      ]
    }
  ]
}